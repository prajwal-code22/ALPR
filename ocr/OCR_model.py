# -*- coding: utf-8 -*-
"""OCR_model.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1QZHlactn9cL271Yb4W4FmWq18viGjL-R

**This code is trains one subset of EMNIST dataset which is trained in one colab account due to time contraint. This same architecture is used to train other subset of datas in other colab account. That is, 4 in this case**
"""


import torch
import torch.nn as nn
from torch.utils.data import DataLoader, Subset

import torch.optim as optim

import torchvision
from torchmetrics import Accuracy, Recall, Precision
from datetime import datetime
import os

from image_transformers import train_transforms, auto_contrast_probability

device = torch.device( 'cuda:0' if torch.cuda.is_available() else 'cpu' )

# Other vars
this_training_session_rank = 1

# Hyperparams
epoch = 4
batch_size = 80
lr=.01
momentum = .9

# Loading train and test dataset
root = "./drive/MyDrive/data"
train_dataset = torchvision.datasets.EMNIST( root=root, train=True, download=True, transform=train_transforms, split="byclass")
test_dataset = torchvision.datasets.EMNIST( root=root, train=False, download=True, transform=train_transforms, split="byclass")
data_to_train_on=len(train_dataset)

data_to_train_on /= 4

# start, end = data_to_train_on*(this_training_session_rank - 1 ), data_to_train_on*this_training_session_rank
# train_dataset = Subset( train_dataset, [ i for i in range(int(start), int(end)) ])

# Dataloaders
train_dataloader = DataLoader( train_dataset, batch_size=batch_size, shuffle=True)
test_dataloader = DataLoader( test_dataset, batch_size=batch_size, shuffle=False)

# Data probing
# next(iter(train_dataloader))
train_dataset[0][0].shape
len(train_dataset)

class OCRModel(nn.Module):
    def __init__(self, num_classes: int):
        super().__init__()

        self.image_conv_net = nn.Sequential(
            nn.Conv2d(1, 32, kernel_size=3, padding=1),  # --> 32x64x64
            nn.ReLU(),
            nn.MaxPool2d(2,2), # --> 32x32x32

            nn.Conv2d(32, 16, kernel_size=3, padding=1), # --> 16x32x32
            nn.ReLU(),
            nn.MaxPool2d(2,2),    # --> 16x16x16

            nn.Conv2d(16, 10, kernel_size=3 , padding=1), # --> 10x16x16
            nn.ReLU(),
            nn.MaxPool2d(2,2), # --> 10x8x8

            nn.Flatten() # --> 10x8x8
        )

        self.fc_layer = nn.Sequential(
            nn.Linear(10*8*8, 10),
            nn.ReLU(),

            nn.BatchNorm1d(10), # Normalizes each batch, that is, tries to make input distribution equal to output distribution.

            nn.Linear(10, 10),
            nn.ReLU(),

            nn.Dropout(.3), # Randomly drops some neurons with prob. of 3% reducing chances of overfitting

            nn.Linear(10, 5),
            nn.ReLU(),

            nn.Linear(5, num_classes)
        )

    def forward(self, features):
        out = self.image_conv_net(features)
        out = self.fc_layer(out)

        return out

# Model initialization
num_classes = test_dataset.classes.__len__()
model = OCRModel(num_classes).to(device)

# Loss and optimizers
criterion = nn.CrossEntropyLoss()
optimizer = optim.SGD(model.parameters(), lr=lr, momentum=momentum)

# Saving model
def save_model(model, epoch: int, final_model: bool=False):
    cdt = datetime.now()
    now = cdt.strftime("%Y-%m-%d_%H_%M_%S")
    saving_dir = "./drive/MyDrive/intermediate-models"

    try:
        os.mkdir(saving_dir)
    except:
        pass
    model_name = f"{epoch}-{now}.pth" if not final_model else f"{epoch}-final.pth"
    torch.save(model, f"{saving_dir}/{model_name}")

import os
from pathlib import Path
saving_dir = Path("./drive/MyDrive/intermediate-models")
last_epoch = 0
try:
    pwd = os.getcwd()
    files = os.listdir(pwd / saving_dir)
    files.sort()
    files = [ f for f in files if f[0].isnumeric() ]
    print(files)
    last_model_file = files[-1]
    last_epoch = int(last_model_file.split("-")[0])
    # epoch -= (int(last_epoch)+1)
    model = torch.load(saving_dir/last_model_file, weights_only=False) if torch.cuda.is_available() else  torch.load(saving_dir/last_model_file, weights_only=False, map_location=torch.device('cpu') )
    print(f"Loaded file {last_model_file}")
    print(f"Continuing remaining {epoch} epoch.")
except Exception as e:
    print(e)

epoch_loss_store = []
# # Training loop
# print(f"Training data from {data_to_train_on*(this_training_session_rank-1)}:{data_to_train_on*this_training_session_rank}")
# for i in range(epoch):
#     running_loss = .0
#     for ind, (features, labels) in enumerate(train_dataloader):

#         features = features.to(device)
#         labels = labels.to(device)

#         # Forward pass and loss calcuation
#         outputs = model(features)
#         loss = criterion(outputs, labels)

#         # Backward pass ans optimizing
#         optimizer.zero_grad()
#         loss.backward()
#         optimizer.step()

#         running_loss += loss.item()

#         if ind % 100 == 0 and ind > 0:
#           print("Saving model...")
#           save_model(model, epoch=i+ind)

#     running_loss /= len(train_dataloader)
#     print(f"Epoch: {i+1}, Loss: {running_loss} ")
#     epoch_loss_store.append(running_loss)

#     # Saving model in every 1 epoch

# Testing loop
print("Testing started...")
all_accuracy = Accuracy(task="multiclass", num_classes=num_classes, average="micro").to(device)
all_precision = Precision(task="multiclass", num_classes=num_classes, average="micro").to(device)
all_recall = Recall(task="multiclass", num_classes=num_classes, average="micro").to(device)

total_loss = .0
model.eval() # Changin model to evaluation mode so it doesnot change weignts.
with torch.no_grad():

    for features, labels in test_dataloader:

        features = features.to(device)
        labels = labels.to(device)


        outputs = model(features)
        loss = criterion(outputs, labels)

        total_loss += loss.item()

        # Metrics
        all_accuracy(outputs, labels)
        all_recall(outputs, labels)
        all_precision(outputs, labels)

average_loss = total_loss / len(test_dataloader)
accuracy = all_accuracy.compute()
recall = all_recall.compute()
precision = all_precision.compute()

print(F"Total loss: {total_loss}")
print(F"Avg loss: {average_loss}")
print(F"{accuracy=}")
print(F"{recall=}")
print(F"{precision=}")

# Saving model

cdt = datetime.now()
now = cdt.strftime("%Y-%m-%d_%H_%M_%S")
saving_dir = Path("./drive/MyDrive/intermediate-models")
with open( saving_dir / f"params-{now}.txt" , "w") as f:
    f.write(f"# Hyperparameters \n")
    f.write(f"{epoch=}\n")
    f.write(f"{batch_size=}\n")
    f.write(f"{lr=}\n")
    f.write(f"{momentum=}\n")
    f.write(f"{data_to_train_on=} datas\n\n")
    f.write(f"{auto_contrast_probability=}\n\n")

    f.write(f"# Metrics \n")
    f.write(f"{average_loss=}\n")
    f.write(f"{total_loss=}\n")
    f.write(f"{recall=}\n")
    f.write(f"{precision=}\n")
    f.write(f"{accuracy=}\n")

    f.write("# Epoch and loss\n")
    for loss in epoch_loss_store:
      f.write(f"{loss}")